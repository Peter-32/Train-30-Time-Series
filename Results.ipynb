{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal\n",
    "\n",
    "Run a 10 small random forest times on each of 30 univariate time series to predict the next 10 days.  Each model predicts one of the days ahead.\n",
    "\n",
    "### Data Sources\n",
    "\n",
    "- https://www.kaggle.com/felixzhao/productdemandforecasting/home\n",
    "- historical_product_demand.csv\n",
    "\n",
    "### Data Descriptions\n",
    "\n",
    "- Descriptions from: https://www.kaggle.com/felixzhao/productdemandforecasting/home\n",
    "- Product_Code: The product name encoded\n",
    "- Warehouse: Warehouse name encoded\n",
    "- Product_Category: Product Category for each Product_Code encoded\n",
    "- Date: The date customer needs the product\n",
    "- Order_Demand: single order qty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tables/3_mse.csv\")\n",
    "f, ax = plt.subplots(figsize=(7, 7))\n",
    "df['mse_average'] = mstats.winsorize(df['mse_average'].values, limits=[0.00, 0.05])\n",
    "df['mse_persistence'] = mstats.winsorize(df['mse_persistence'].values, limits=[0.00, 0.04])\n",
    "df['mse_rf'] = mstats.winsorize(df['mse_rf'].values, limits=[0.00, 0.05])\n",
    "sns.violinplot(data=df, ax=ax, inner=None)\n",
    "sns.swarmplot(data=df, ax=ax, color='w', alpha=.7);\n",
    "ax.set_title(\"\"\"MSE for 30 Models For 3 Models\n",
    "(removed outlier MSE values)\n",
    "Average mse_average: {}\n",
    "Average mse_persistence: {}\n",
    "Average mse_rf: {}\n",
    "\"\"\".format(np.round(df['mse_average'].mean(),3),\n",
    "           np.round(df['mse_persistence'].mean(),3), \n",
    "           np.round(df['mse_rf'].mean(),3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "These are the predictions and actuals on the test set.\n",
    "![predictions](images/4_predictions.png)\n",
    "\n",
    "These are MSE values when trying 32 regression models on this dataset.  Many of them share similar scores, so I chose random forest.\n",
    "![models](images/models.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL\n",
    "I created a model_key by combining three columns.  I filter down to the 30 model_keys with the most data.  The result is saved so this can be skipped next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_etl():\n",
    "    df = pd.read_csv(\"source_data/historical_product_demand.csv\")\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')\n",
    "    df = sqldf(\"\"\"\n",
    "                SELECT product_code || \"-\" || warehouse || \"-\" || product_category AS model_key, date, Sum(IFNULL(order_demand, 0)) AS Order_Demand\n",
    "                FROM df\n",
    "                GROUP BY 1,\n",
    "                         2 \"\"\", locals())\n",
    "    df = sqldf(\"\"\"\n",
    "                SELECT a.*\n",
    "                FROM df a\n",
    "                INNER JOIN\n",
    "                  (SELECT model_key,\n",
    "                          Count(*) n\n",
    "                   FROM df\n",
    "                   GROUP BY 1\n",
    "                   ORDER BY 2 DESC, 1\n",
    "                   LIMIT 30) b ON b.model_key = a.model_key\n",
    "                ORDER BY model_key, Date\"\"\", locals())\n",
    "    df.to_csv(\"tables/1_thirty_models.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration\n",
    "I plot the 30 models to visualize the dataset.  I used Excel to find the min and max date.  I decided there are outliers to remove and that I won't log the data.  I decided to fix the missing data with a 7 day moving average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_exploration():\n",
    "    df = pd.read_csv(\"tables/1_thirty_models.csv\")\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.set_index([\"Date\"])\n",
    "    thirty_plots(df=df, filename=\"1_original_data\")\n",
    "    # Good min/max dates: 1/5/12 and 12/28/16\n",
    "    # Will cap at 95% upper bound and 5% lower bound\n",
    "    # Will not log the data\n",
    "    # Will impute using 7 day moving average\n",
    "    # There are lots of missing values\n",
    "    print(\"There are infinity values: {}\".format(\n",
    "        df.fillna(0).replace([np.inf, -np.inf], np.nan).isnull().values.any()))\n",
    "    # There are no infinity values\n",
    "\n",
    "\n",
    "def thirty_plots(df, filename):\n",
    "    df = pd.pivot_table(data=df, values='Order_Demand',\n",
    "                        index=\"Date\", columns=\"model_key\")\n",
    "    df.to_csv(\"tables/_row_date__col_product.csv\")\n",
    "    colors = ['#A8E6CE', '#A8E6CE', '#A8E6CE',\n",
    "              '#DCEDC2', '#DCEDC2', '#DCEDC2',\n",
    "              '#FFD3B5', '#FFD3B5', '#FFD3B5',\n",
    "              '#FFAAA6', '#FFAAA6', '#FFAAA6',\n",
    "              '#FF8C94',  '#FF8C94', '#FF8C94',\n",
    "              '#A8E6CE', '#A8E6CE', '#A8E6CE',\n",
    "              '#DCEDC2', '#DCEDC2', '#DCEDC2',\n",
    "              '#FFD3B5', '#FFD3B5', '#FFD3B5',\n",
    "              '#FFAAA6', '#FFAAA6', '#FFAAA6',\n",
    "              '#FF8C94',  '#FF8C94', '#FF8C94']\n",
    "    fig, ax = plt.subplots(nrows=10, ncols=3)\n",
    "    for i, ax in enumerate(ax.flatten()):\n",
    "        df[df.columns[i]].plot(color=colors[i], ax=ax, sharex=ax)\n",
    "        # ax.set_title(df.columns[i], fontsize=2)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=4)\n",
    "        ax.tick_params(axis='both', which='minor', labelsize=2)\n",
    "    plt.savefig('images/{}.png'.format(filename))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I loop through all model keys.  I fill in the missing days, remove outliers, impute the data, and add lag terms.\n",
    "\n",
    "##### Before\n",
    "![](images/1_original_data.png)\n",
    "##### After\n",
    "![](images/3_prepared_data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_prepare_data():\n",
    "    main_df = pd.read_csv(\"tables/1_thirty_models.csv\")\n",
    "    model_keys = main_df['model_key'].unique().tolist()\n",
    "    prepared_df = pd.DataFrame()\n",
    "    for model_key in model_keys:\n",
    "        df = load_data(main_df=main_df, model_key=model_key)\n",
    "        df = fill_missing_days__and_set_datetime_index(\n",
    "            df, start_date=\"2012-01-05\", end_date=\"2016-12-28\")\n",
    "        df.to_csv(\"tables/_fill_days.csv\")\n",
    "        df.loc[:, 'Order_Demand'] = mstats.winsorize(\n",
    "            df['Order_Demand'].values, limits=[0.05, 0.05])\n",
    "        # df['Order_Demand'] = df['Order_Demand'].apply(lambda x: np.log(x+1))\n",
    "        df = moving_average_imputation(df)\n",
    "        plot(df, model_key, '2_imputation_example')\n",
    "        df['model_key'] = df['Order_Demand'].apply(lambda x: model_key)\n",
    "        for i in range(1, 9):\n",
    "            df['lag_{}'.format(i)] = df['Order_Demand'].shift(i)\n",
    "        df = df.dropna()\n",
    "        df['Date'] = df.index\n",
    "        prepared_df = pd.concat([prepared_df, df])\n",
    "    prepared_df = sqldf(\n",
    "        \"select model_key, Date, Order_Demand, lag_1, lag_2, lag_3, lag_4, lag_5, lag_6, lag_7, lag_8 from prepared_df ORDER BY 1,2\", locals())\n",
    "    prepared_df['Date'] = pd.to_datetime(prepared_df['Date'])\n",
    "    prepared_df.to_csv(\"tables/2_data_prepared.csv\", index=False)\n",
    "    prepared_df.set_index(['Date'], inplace=True)\n",
    "    thirty_plots(df=prepared_df, filename=\"3_prepared_data\")\n",
    "\n",
    "\n",
    "def load_data(main_df, model_key):\n",
    "    return sqldf(\"SELECT Date, Order_Demand from main_df where model_key = '{}' ORDER BY 1\".format(model_key), locals())\n",
    "\n",
    "\n",
    "def fill_missing_days__and_set_datetime_index(df, start_date, end_date):\n",
    "    idx = pd.date_range(start_date, end_date)\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "    df = df.reindex(idx, fill_value=0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def moving_average_imputation(df):\n",
    "    # Consider +/- infinity here if working on a different time series\n",
    "    df = df.replace(0, np.nan)\n",
    "    df['Order_Demand'] = df['Order_Demand'].fillna(\n",
    "        df['Order_Demand'].rolling(window=7, min_periods=1, center=False).mean())\n",
    "    df['Order_Demand'] = df['Order_Demand'].fillna(method='ffill')\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot(df, model_key, filename):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(model_key, fontsize=12)\n",
    "    plt.plot(df['Order_Demand'])\n",
    "    plt.savefig('images/{}.png'.format(filename))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spot Check Algorithms\n",
    "I loop through the model keys, get y, split the data, and run random forest.  I plot it and save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_spot_check_algorithms():\n",
    "    main_df = pd.read_csv(\"tables/2_data_prepared.csv\")\n",
    "    model_keys = main_df['model_key'].unique().tolist()\n",
    "    mse_df = pd.DataFrame()\n",
    "    fig = plt.figure()\n",
    "    i = 0\n",
    "    for model_key in model_keys:\n",
    "        i = i + 1\n",
    "        df = load_data_2(main_df=main_df, model_key=model_key)\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df.set_index('Date', inplace=True)\n",
    "        df['y'] = df['Order_Demand'].shift(-1)\n",
    "        df = df.dropna()\n",
    "        train, test = get_train_test_split(df)\n",
    "\n",
    "        train_X = train.drop(['y'], axis=1).values\n",
    "        scaler = StandardScaler()\n",
    "        train_X = scaler.fit_transform(train_X)\n",
    "        train_y = train['y'].values\n",
    "        test_X = test.drop(['y'], axis=1).values\n",
    "        test_X = scaler.transform(test_X)\n",
    "        test_y = test['y'].values\n",
    "\n",
    "        model = ensemble.RandomForestRegressor(\n",
    "            n_estimators=50, max_features=\"log2\", min_samples_leaf=5, criterion=\"mse\", bootstrap=True, random_state=2)\n",
    "        model = model.fit(train_X, train_y)\n",
    "        pred_y = model.predict(test_X)\n",
    "        y_average = np.ones(10) * train['Order_Demand'].mean()\n",
    "\n",
    "        mse_persistence = mean_absolute_percentage_error(\n",
    "            test['y'], test['Order_Demand'])\n",
    "        mse_average = mean_absolute_percentage_error(test['y'], y_average)\n",
    "        mse_rf = mean_absolute_percentage_error(test['y'], pred_y)\n",
    "\n",
    "        d = {'mse_persistence': [mse_persistence],\n",
    "             'mse_average': [mse_average], 'mse_rf': [mse_rf]}\n",
    "        df_temp = pd.DataFrame(data=d)\n",
    "        mse_df = pd.concat([mse_df, df_temp])\n",
    "\n",
    "        ax = fig.add_subplot(10, 3, i)\n",
    "        ax.plot(test['y'].values)\n",
    "        ax.plot(pred_y)\n",
    "        ax.plot(y_average)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=4)\n",
    "        ax.tick_params(axis='both', which='minor', labelsize=2)\n",
    "        if i == 1:\n",
    "            ax.legend(['y_test', 'y_pred', 'y_average'],\n",
    "                      loc='lower left', fontsize=4)\n",
    "    plt.savefig('images/4_predictions.png')\n",
    "    mse_df.to_csv(\"tables/3_mse.csv\", index=False)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def load_data_2(main_df, model_key):\n",
    "    return sqldf(\"SELECT Date, Order_Demand, lag_1, lag_2, lag_3, lag_4, lag_5, lag_6, lag_7, lag_8 from main_df where model_key = '{}' ORDER BY 1\".format(model_key), locals())\n",
    "\n",
    "\n",
    "def get_train_test_split(df):\n",
    "    days_in_test_set = 10\n",
    "    split_point = len(df) - days_in_test_set\n",
    "    train, test = df[0:split_point], df[split_point:]\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    # y_true, y_pred = np.expm1(y_true), np.expm1(y_pred)\n",
    "    return np.mean(np.abs(y_true - y_pred) / np.abs(y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main\n",
    "- Each function loads data from CSV, so you can comment out earlier steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL\n",
    "do_etl()\n",
    "\n",
    "\n",
    "# Exploration\n",
    "do_exploration()\n",
    "\n",
    "\n",
    "# Prepare data\n",
    "do_prepare_data()\n",
    "\n",
    "\n",
    "# Spot Check Algorithms\n",
    "do_spot_check_algorithms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas for Improving the Model\n",
    "- Verify that the data is stationary\n",
    "- Try Auto ARIMA and simpler forecasting methods\n",
    "- Try Kaggle ideas for feature engineering with ML models\n",
    "- Grid Search\n",
    "- Ensemble\n",
    "- Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "# import missingno as msno\n",
    "from pandasql import sqldf\n",
    "from scipy.stats import mstats\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import linear_model, neighbors, tree, svm, ensemble\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from tpot.builtins import StackingEstimator\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from scipy.stats import boxcox\n",
    "# from scipy.special import inv_boxcox"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "commons",
   "language": "python",
   "name": "commons"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
